{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return torch.device('mps')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "import unittest\n",
    "\n",
    "def make_src_mask(src,src_pad_idx,device=None):\n",
    "    \"\"\"\n",
    "    Creates a mask for the source input sequence.\n",
    "    \n",
    "    Args:\n",
    "        src (torch.Tensor): The source input sequence.\n",
    "    \n",
    "    Returns:\n",
    "        src_mask (torch.Tensor): The mask for the source input sequence.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "    src_mask = (src != src_pad_idx).unsqueeze(1).unsqueeze(2).to(device)\n",
    "    return src_mask\n",
    "\n",
    "def make_trg_mask(trg,device=None):\n",
    "    \"\"\"\n",
    "    Creates a mask for the target input sequence.\n",
    "    \n",
    "    Args:\n",
    "        trg (torch.Tensor): The target input sequence.\n",
    "    \n",
    "    Returns:\n",
    "        trg_mask (torch.Tensor): The mask for the target input sequence.\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = get_device()\n",
    "    N, trg_len = trg.shape\n",
    "    trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "        N, 1, trg_len, trg_len\n",
    "    ).to(device)\n",
    "    \n",
    "    return trg_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer from scratch\n",
    "\n",
    "Writing a Transformer from scratch is an educational journey into the inner workings of one of the most influential models in the field of Natural Language Processing. Here's why focusing on this process can be so beneficial:\n",
    "\n",
    "**Deep Understanding**: Utilizing pre-trained models or libraries can often lead to a surface-level understanding of the underlying mechanisms. On the other hand, creating a Transformer model from the ground up facilitates a deep, granular comprehension of each part of this complex architecture. You'll intimately understand the self-attention mechanism, positional encoding, layer normalization, and the importance of masks, among other concepts, as these are all pieces you'll have to create and interconnect.\n",
    "\n",
    "**Educational Value**: Implementing a Transformer from scratch is akin to a hands-on masterclass in advanced deep learning. It's an effective method of learning that goes beyond theory, encouraging you to apply knowledge and solve problems as they arise. It enables you to grapple with the practical aspects of model development, including debugging, ensuring computational efficiency, and managing resourcesâ€”a real-life skill set often omitted in theoretical coursework.\n",
    "\n",
    "**Deeper Insights into NLP**: Transformers form the backbone of modern NLP, powering applications from translation to text generation. By building a Transformer yourself, you'll gain insights into why they're so effective for handling language data and how different components contribute to their exceptional performance. This can be a stepping stone to further explorations and innovations in NLP.\n",
    "\n",
    "**Insight into Generative AI**: Transformers have revolutionized not just NLP, but the broader domain of generative AI as well. They lie at the core of cutting-edge models like GPT-3 that can generate human-like text, and have also found applications in areas like music and image generation. By constructing a Transformer from scratch, you'll gain a foundational understanding of the principles that drive these powerful generative models. This equips you to participate in and contribute to the ongoing advancements in this rapidly evolving field, whether by optimizing existing generative models or pioneering new approaches.\n",
    "\n",
    "In conclusion, while using pre-built models has its advantages, nothing quite matches the depth of understanding and learning achieved by building a Transformer model from scratch. It's an endeavor that requires dedication and effort, but the resultant knowledge and skills gained make it a worthwhile investment for any serious learner or practitioner in the field of NLP.\n",
    "\n",
    "![](imgs/attention-enc-decoder.png)\n",
    "\n",
    "I have decided to embark on this journey of building a Transformer from scratch, inspired by the significant advancements and prolific literature available in the field. There's a rich body of resources and scholarly articles, tutorials, and code repositories that break down the complexity of the Transformer architecture, serving as invaluable guides and references. I'm particularly grateful to those brave pioneers who ventured into uncharted territory and shared their experiences and insights. Their work has illuminated the path for others and has been instrumental in making these complex concepts more accessible to the broader AI community. Through this endeavor, I hope to deepen my own understanding of Transformer models, while also contributing to the growing body of knowledge on this subject.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Started with Transformers\n",
    "\n",
    "I am familiar with [Pytorch](https://pytorch.org/) and I will be using it to build the Transformer, but you can use any framework you prefer. As a developer, I find it helpful to have a clear plan of action before starting a project. This helps me stay focused and motivated, and also ensures that I don't get lost in the details. The famous paper ([Attention is all you need](https://arxiv.org/abs/1706.03762)) is a must read. I will be using the same notation as the paper as much as possible, so it's important to understand the terminology and notation used in the paper. I will also be using the same variable names as the paper, so it's helpful to have the paper open as a reference while going thru is Jupyter Notebook. Here's the high-level plan I've laid out for this project:\n",
    "\n",
    "* I will start by building the core components of the Transformer architecture, which I will combine to create the complete the Transformer model. From a high-level I have decided to build the following components:\n",
    "\n",
    "    * Multi-head attention\n",
    "    * Transformer Block (which is used in Encoder and Decoder)\n",
    "    * Encoder \n",
    "    * Decoder block which uses the Transformer Block and Multi-head attention\n",
    "    * Decoder \n",
    "    * And finally Transformer which uses the Encoder and Decoder\n",
    "\n",
    "\n",
    "* Every component mentioned above will be:\n",
    "  * Defined as a [`nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html) subclass. This will allow me to leverage the built-in capabilities of PyTorch, including the ability to track and update the model parameters, compute gradients, and more.\n",
    "  * Have a corresponding unit test to ensure that it's working as expected. I will also use the unit tests to debug the code and fix any bugs that arise. \n",
    "  * Have a [`mermaid`](https://mermaid.js.org/intro/) diagram to visualize the flow of data. I will use the diagrams to ensure that the dimensions of tensors match at each step, and as a visual aid to understand the model architecture.\n",
    "\n",
    "\n",
    "![](imgs/TrransformerFromScratch.png)\n",
    "\n",
    "Without further ado, let's begin!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But before we start, how does the data flow in a Transformer?\n",
    "\n",
    "Assume you have a trained Transformer model that can translate English sentences to French. Here's how the data flows through the model during prediction:\n",
    "\n",
    "1. **Source Token Sequence**: This is your input sentence in English, tokenized into a sequence of word vectors. The model reads this sequence to understand the context of the input sentence. Assuming N=1 (N is the batch number or the number of sentences in the input), the shape of the tensor is (1, SeqLength), where SeqLength is the length of the sequence or the number of tokens in the English sentence.\n",
    "\n",
    "1. **Source Mask**: This is applied to the source sequence. The purpose of the source mask is to prevent the attention mechanism from focusing on irrelevant parts of the input, such as padding tokens. In a batch of sequences, not all sequences will be of the same length. To make them uniform in length, padding tokens are added to the shorter sequences. These padded positions don't carry any meaningful information, and it's important to prevent the self-attention mechanism from considering these positions. A padding mask is used for this purpose. It is typically a tensor with values that are either 0 (for padding positions) or 1 (for actual data positions).\n",
    "\n",
    "1. **Encoder**: The encoder processes the source token sequence and the source mask. Since we are in inference mode, the model isn't being trained, so the weights remain the same. The encoder's role is to create a rich representation of the source sentence that captures the semantic information of the input.\n",
    "\n",
    "1. **Target Token Sequence**: At the start of the decoding process, this sequence contains only a start-of-sequence token (in this case for the French language translation). The goal of the model is to generate the rest of the sequence, token by token, which will form our translated sentence in French.\n",
    "\n",
    "1. **Target Mask**: This mask ensures that the prediction for each position can depend only on known outputs at earlier positions, not on future positions, maintaining the auto-regressive property.\n",
    "\n",
    "1. **Decoder**: The decoder processes the output from the encoder, the initial target token sequence, and the target and source masks. It then generates a probability distribution for the next token (the next word in the French translation).The token (word) with the highest probability is selected and added to the target sequence. This updated target sequence is then fed back into the decoder, and the process is repeated. This generation continues until an end-of-sequence token is produced or the sequence reaches a predefined maximum length.\n",
    "\n",
    "This way, during inference/prediction mode, our English-to-French Transformer model generates a French translation of the input English sentence, one word at a time, based on both the original English sentence and the French words it has generated so far.\n",
    "\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "SRC[Source Token Sequence] --shape=N,SeqLength-->  ENC[Encoder]\n",
    "SRCM[Source Mask] --> ENC\n",
    "ENC --shape=N,SeqLength,E--> DEC[Decoder]\n",
    "TRG[Target Token Sequence] --shape=N,SeqLength-->  DEC\n",
    "SRCM --> DEC\n",
    "TRGM[Target Mask] --> DEC\n",
    "DEC --shape=N,SeqLength,E--> O[Output]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multti-ead Self Attention Block\n",
    "\n",
    "In machine learning, particularly in models such as Transformers, an attention function is a crucial component that enhances the model's ability to focus on specific parts of the input data that are relevant for a given task.\n",
    "\n",
    "Here's an expanded explanation of the key concepts:\n",
    "\n",
    "**Query, Keys, and Values**: These are terms used in the attention mechanism. The 'query' is a vector that represents the current item or context we're focusing on. 'Keys' are vectors that represent the different items or contexts available in the input data. 'Values' are vectors that represent the actual content or information associated with each key. They query and keys always have the same dimension, but the values can have the same or different dimension. \n",
    "\n",
    "**Compatibility Function**: The weights assigned to each value in the weighted sum are determined by a 'compatibility function'. This is a function that takes the query and a key as inputs, and outputs a score that represents how well the query and key match. This score is then used as the weight for the corresponding value in the weighted sum. In other words, if the query matches well with a certain key (i.e., the compatibility function gives a high score), then the corresponding value gets a high weight, and therefore has a larger influence on the output of the attention function. This is how the attention mechanism allows the model to focus on the most relevant parts of the input data.\n",
    "\n",
    "**Attention Function**: This is essentially a mapping procedure. It maps a query (a representation of the current context or focus) and a set of key-value pairs (representations of the possible inputs and their corresponding outputs or 'values') to an output. \n",
    "\n",
    "**Weighted Sum of Values**: The output of the attention function is calculated as a weighted sum of the values. This means each value vector is multiplied by a certain weight, and then all of these weighted values are added together to produce the output. The idea is to give more 'attention' (i.e., higher weight) to the values that are more relevant or useful for the current task.\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "\n",
    "![](imgs/mha.png)\n",
    "\n",
    "\n",
    "**Multi-Head Attention**: The multi-head attention mechanism is a variant of the attention mechanism that allows the model to jointly attend to information from different embedding/representation subspaces at different positions. This is beneficial as it allows the model to learn more robust relationships between the query and the key-value pairs. It also helps the model to focus on different positions and subspaces of the input data at different layers of the model.\n",
    "\n",
    "$$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W_O$$\n",
    "$$\\text{where} \\quad \\text{head}_i = \\text{Attention}(QW_{Qi}, KW_{Ki}, VW_{Vi})$$\n",
    "\n",
    "The projections are defined by parameter matrices: \n",
    "\n",
    "- $W_{Qi} \\in \\mathbb{R}^{E_{\\text{model}} \\times E_q}$ \n",
    "- $W_{Ki} \\in \\mathbb{R}^{E_{\\text{model}} \\times E_k}$ \n",
    "- $W_{Vi} \\in \\mathbb{R}^{E_{\\text{model}} \\times E_v}$ \n",
    "- $W_O \\in \\mathbb{R}^{h E_v \\times E_{\\text{model}}}$\n",
    "\n",
    "In our implementation, we use $h=8$ parallel attention layers, also known as heads. For each of these, we set \n",
    "\n",
    "$E_k = E_q = E_v = E_{\\text{model}} / h = 64$. \n",
    "\n",
    "\n",
    "#### Input\n",
    "1. **Values(N,Sv,E)**, **Keys(N,Sk,E)**, **Queries(N,Sq,E)**: \n",
    "    they are three tensors \"Values\", \"Keys\", \"Queries\" which are linearly transformed to the same embedding size \"E\". Keys and Queries have the same shape, while Values can have a different shape (the number of sequences \"Sv\" can be different from the number of sequences in Keys and Queries \"Sk\")\n",
    "    * **N**: Batch Size\n",
    "    * **Sv**, **Sk**, **Sq**: Sequence Lengths of Values, Keys, Queries\n",
    "    * **E**: Embedding Size\n",
    "2. **Mask** (Optional)one of two types:\n",
    "    * **Target Mask**: The mask is used to prevent the model from attending to the future tokens. It is a tensor of shape (N, 1, Sq, Sq) where the future tokens are set to 0 and the rest to 1.\n",
    "    * **Source Mask**: The mask is used to prevent the model from attending to the padding tokens. It is a tensor of shape (N, 1,1,Sq/Sk) where the padding tokens are set to 0 and the rest to 1. \n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "\n",
    "V[Values] -- shape=N,Sv,E--> B[Linear]\n",
    "K[Keys] --shape=N,Sk,E--> C[Linear]\n",
    "Q[Queries] --shape=N,Sq,E--> D[Linear]\n",
    "B --Values=N,Sv,E--> E[Reshape Values to N,Sv,H,Hd]\n",
    "C --Keys=N,Sk,E--> F[Reshape Keys to N,Sk,H,Hd]\n",
    "D --Queries=N,Sq,E--> G[Reshape Queries to N,Sq,H,Hd]\n",
    "F --Keys=N,Sk,H,Hd,--> H[Scaled Dot Product]\n",
    "G --Queries=N,Sq,H,Hd--> H\n",
    "H --Queries=N,Sq,H,Hd--> U[Mask]\n",
    "U --> I[Softmax]\n",
    "I --Softmax=N,H,Sq,Sk--> J[Dot Product]\n",
    "E --Values=N,Sv,H,Hd--> J\n",
    "J --Dot Product=N,H,Sq,Sk--> L[Reshape to N,Sq,E or concatenate the heads]\n",
    "L --MatchingValues=N,Sq,E--> M[Linear]\n",
    "M --MatchingValues=N,Sq,E--> N[Output]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadSelfAttentionBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Initializes the MultiHeadSelfAttentionBlock module.\n",
    "\n",
    "    Args:\n",
    "        embedding_size (int): The size of the input embeddings. The value should be \n",
    "                            divisible by num_heads. This is because the embeddings \n",
    "                            are split into `num_heads` different pieces during the \n",
    "                            self-attention process.\n",
    "        num_heads (int): The number of attention heads. In the multi-head attention \n",
    "                        mechanism, the model generates `num_heads` different attention \n",
    "                        scores for each token in the sequence. This allows the model \n",
    "                        to focus on different parts of the sequence for each token.\n",
    "\n",
    "    Raises:\n",
    "        AssertionError: If `embedding_size` is not divisible by `num_heads`. The embedding \n",
    "                        size needs to be divisible by the number of heads to ensure even \n",
    "                        division of embeddings for multi-head attention.\n",
    "    \"\"\"    \n",
    "    def __init__(self, embedding_size, num_heads,device=None):\n",
    "        super(MultiHeadSelfAttentionBlock, self).__init__()\n",
    "        if device is None:\n",
    "            self.device = get_device()\n",
    "        else:\n",
    "            self.device = device\n",
    "        self.embedding_size = embedding_size\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embedding_size // num_heads\n",
    "\n",
    "        assert (\n",
    "            self.head_dim * num_heads == embedding_size\n",
    "        ), \"embedding_size  needs to be divisible by num_heads\"\n",
    "\n",
    "        # Define the linear transformations for the input data \n",
    "        self.values_transform = nn.Linear(embedding_size, embedding_size)\n",
    "        self.keys_transform = nn.Linear(embedding_size, embedding_size)\n",
    "        self.queries_transform = nn.Linear(embedding_size, embedding_size)\n",
    "        # Define the final output linear transformation\n",
    "        self.linear_out = nn.Linear(embedding_size, embedding_size)\n",
    "\n",
    "    def forward(self, values, keys, queries, mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the MultiHeadSelfAttentionBlock module.\n",
    "\n",
    "        Args:\n",
    "            values (torch.Tensor): The values tensor of shape (N, value_len, embedding_size),\n",
    "                where N is the batch size, value_len is the sequence length for the values.\n",
    "            keys (torch.Tensor): The keys tensor of shape (N, key_len, embedding_size), \n",
    "                where key_len is the sequence length for the keys.\n",
    "            queries (torch.Tensor): The query tensor of shape (N, query_len, embedding_size), \n",
    "                where query_len is the sequence length for the queries.\n",
    "            mask (torch.Tensor, optional): A mask tensor of shape (N, 1, 1, query_len/key_len),\n",
    "                where the values are either 1 (for positions to be attended to) or 0 \n",
    "                (for positions to be masked). The mask is used to prevent attention to \n",
    "                certain positions. Default is None.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor of shape (N, query_len, embedding_size), \n",
    "                where N is the batch size, query_len is the sequence length for the queries,\n",
    "                and embedding_size is the dimension of the output embeddings. \n",
    "                This tensor represents the result of applying self-attention on the input.\n",
    "\n",
    "        Note:\n",
    "            The method first transforms the input tensors (values, keys, query) using \n",
    "            separate linear transformations. Then, it splits the transformed embeddings \n",
    "            into multiple heads and computes the attention scores (queries * keys). If a mask \n",
    "            is provided, it is applied to the scores. The scores are then normalized to create \n",
    "            the attention weights. The method then computes the weighted sum of the values \n",
    "            (attention * values), applies a final linear transformation, and returns the result.\n",
    "        \"\"\"        \n",
    "        # Get the number of training examples\n",
    "        num_examples = queries.shape[0]\n",
    "\n",
    "        # Calculate sequence lengths\n",
    "        value_len, key_len, query_len = values.shape[1], keys.shape[1], queries.shape[1]\n",
    "        \n",
    "        values = values.to(self.device)\n",
    "        keys = keys.to(self.device)\n",
    "        queries = queries.to(self.device)\n",
    "        mask = mask.to(self.device) if mask is not None else None\n",
    "\n",
    "        # Transform the input data\n",
    "        values = self.values_transform(values)\n",
    "        keys = self.keys_transform(keys)\n",
    "        queries = self.queries_transform(queries)\n",
    "\n",
    "        # Split the embeddings into multiple heads\n",
    "        values = values.reshape(num_examples, value_len, self.num_heads, self.head_dim)\n",
    "        keys = keys.reshape(num_examples, key_len, self.num_heads, self.head_dim)\n",
    "        queries = queries.reshape(num_examples, query_len, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Compute attention scores (queries * keys)\n",
    "        attention_scores = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys])\n",
    "        # queries shape: (N, query_len, num_heads, heads_dim),\n",
    "        # keys shape: (N, key_len, num_heads, heads_dim)\n",
    "        # attention_scores: (N, num_heads, query_len, key_len)        \n",
    "\n",
    "        # Apply the mask to the scores\n",
    "        # mask shape: (N, 1, 1, query_len/key_len)\n",
    "        if mask is not None:\n",
    "            attention_scores = attention_scores.masked_fill(mask == 0, float(\"-1e20\"))\n",
    "\n",
    "        # Normalize the scores\n",
    "        # attention shape: (N, num_heads, query_len, key_len)\n",
    "        attention = torch.softmax(attention_scores / (self.embedding_size ** 0.5), dim=3)\n",
    "\n",
    "        # Compute weighted values (attention * values)\n",
    "        # attention shape: (N, num_heads, query_len, key_len)\n",
    "        # values shape: (N, value_len, num_heads, heads_dim)\n",
    "        # weighted_values shape: (N, query_len, num_heads, heads_dim)\n",
    "        weighted_values = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values])\n",
    "        weighted_values = weighted_values.reshape(num_examples, query_len, self.num_heads * self.head_dim)\n",
    "\n",
    "        # Apply the final linear transformation\n",
    "        out = self.linear_out(weighted_values)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "....\n",
      "----------------------------------------------------------------------\n",
      "Ran 4 tests in 0.152s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Test it\n",
    "class TestSelfAttention(unittest.TestCase):\n",
    "\n",
    "    def test_init(self):\n",
    "        # Test that the object initializes correctly\n",
    "        try:\n",
    "            self_attention = MultiHeadSelfAttentionBlock(embedding_size=512, num_heads=8)\n",
    "        except Exception as e:\n",
    "            self.fail(f\"Initialization of SelfAttention failed with {e}\")\n",
    "\n",
    "    def test_forward_pass_small(self):\n",
    "        # Create a SelfAttention object\n",
    "        self_attention = MultiHeadSelfAttentionBlock(embedding_size=6, num_heads=2)\n",
    "        self_attention.to(get_device())\n",
    "\n",
    "        # Create dummy inputs for the forward pass\n",
    "        # 64 examples, 20 tokens, 512 embedding size\n",
    "        N, sequence_length = 1, 5  # Number of examples and sequence length\n",
    "        values = torch.rand((N, sequence_length, 6))\n",
    "        keys = torch.rand((N, sequence_length, 6))\n",
    "        query = torch.rand((N, sequence_length, 6))\n",
    "        mask = torch.ones((N, 1,1,sequence_length))\n",
    "\n",
    "        # Test the forward pass\n",
    "        try:\n",
    "            output = self_attention(values, keys, query, mask)\n",
    "        except Exception as e:\n",
    "            self.fail(f\"Forward pass of SelfAttention failed with {e}\")\n",
    "\n",
    "\n",
    "    def test_forward_pass(self):\n",
    "        # Create a SelfAttention object\n",
    "        self_attention = MultiHeadSelfAttentionBlock(embedding_size=512, num_heads=8)\n",
    "        self_attention.to(get_device())\n",
    "\n",
    "        # Create dummy inputs for the forward pass\n",
    "        # 64 examples, 20 tokens, 512 embedding size\n",
    "        N, sequence_length = 64, 20  # Number of examples and sequence length\n",
    "        values = torch.rand((N, sequence_length, 512))\n",
    "        keys = torch.rand((N, sequence_length, 512))\n",
    "        query = torch.rand((N, sequence_length, 512))\n",
    "        mask = torch.ones((N, 1,1,sequence_length))\n",
    "\n",
    "        # Test the forward pass\n",
    "        try:\n",
    "            output = self_attention(values, keys, query, mask)\n",
    "        except Exception as e:\n",
    "            self.fail(f\"Forward pass of SelfAttention failed with {e}\")\n",
    "\n",
    "    def test_output_shape(self):\n",
    "        # Create a SelfAttention object\n",
    "        self_attention = MultiHeadSelfAttentionBlock(embedding_size=512, num_heads=8)\n",
    "        self_attention.to(get_device())\n",
    "\n",
    "        # Create dummy inputs for the forward pass\n",
    "        N, sequence_length = 64, 20  # Number of examples and sequence length\n",
    "        values = torch.rand((N, sequence_length, 512))\n",
    "        keys = torch.rand((N, sequence_length, 512))\n",
    "        query = torch.rand((N, sequence_length, 512))\n",
    "        mask = torch.ones((N, 1,1,sequence_length))\n",
    "\n",
    "        # Perform the forward pass and check the output shape\n",
    "        output = self_attention(values, keys, query, mask)\n",
    "        self.assertEqual(output.shape, (N, sequence_length, 512),\n",
    "                         \"Output shape of SelfAttention forward pass is incorrect\")\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Block\n",
    "\n",
    "The forward pass in the Transformer Block is as follows:\n",
    "\n",
    "1. **MultiHeadSelfAttentionBlock (MHA)**: `Queries (Q)`, `Keys (K)`, `Values (V)`, and a `Mask (M)` are inputs to the multi-head self-attention block. The shapes of Q, K, and V are (N, Sq, E), (N, Sk, E), and (N, Sv, E), respectively, where N is the batch size, Sq and Sk are sequence lengths of queries and keys and are typically equal, and E is the embedding size. The mask has shape (N, 1, Sq) or (N, 1, 1, Sq), depending on the mask type. This block computes a weighted sum of the 'Values' based on the similarity of 'Keys' and 'Queries', scaled by the mask.\n",
    "\n",
    "1. **LayerNorm + Dropout (LN1)**: The output from the MHA and the original Queries (residual connection) are added together and then passed through the first Layer Normalization and Dropout operation to stabilize the outputs and prevent overfitting. The output of this stage retains the shape (N, Sq, E).\n",
    "\n",
    "1. **FeedForwardBlock (FFN)**: The output of LN1 is passed through a Feed-Forward Neural Network, which consists of two linear transformations with a ReLU activation in between. It's here where the model can learn more complex representations.\n",
    "\n",
    "1. **LayerNorm + Dropout (LN2)**: The output from the FFN and the output of LN1 (again, a residual connection) are added together, then passed through the second Layer Normalization and Dropout operation. This further stabilizes the outputs and helps prevent overfitting.\n",
    "\n",
    "1. **Output (O)**: The output of LN2 becomes the final output of this Transformer block. It has the same shape as the input queries (N, Sq, E) and can be used as input to the next Transformer block in the stack (if any), or can be used for further processing (like a linear transformation to obtain prediction scores for downstream tasks).\n",
    "\n",
    "This high-level description covers one Transformer block, and in practice, several of these blocks are stacked to form the complete Transformer model. This stacking allows the model to learn more complex patterns and relationships in the data.\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "Q[Queries] --shape=N,Sq,E--> MHA\n",
    "V[Values] -- shape=N,Sv,E--> MHA[MultiHeadSelfAttentionBlock]\n",
    "K[Keys] --shape=N,Sk,E--> MHA\n",
    "M[Mask] --shape=N,1,Sq-or-N,1,1,Sq--> MHA\n",
    "Q --Queries=N,Sq,E--> LN1[LayerNorm + Dropout]\n",
    "MHA --Queries=N,Sq,E--> LN1\n",
    "LN1 --Queries=N,Sq,E--> FFN[FeedForwardBlock with forward expansion]\n",
    "FFN --Queries=N,Sq,E--> LN2[LayerNorm + Dropout]\n",
    "LN1 --Queries=N,Sq,E--> LN2\n",
    "LN2 --Queries=N,Sq,E--> O[Output]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A Transformer Block class that defines a single block in a transformer model.\n",
    "\n",
    "    Args:\n",
    "        embed_size (int): The dimensionality of the input embeddings.\n",
    "        num_heads (int): The number of attention heads for the self-attention mechanism.\n",
    "        dropout_rate (float): The dropout rate used in the dropout layers to prevent overfitting.\n",
    "        forward_expansion (int): The factor by which the dimensionality of the input is \n",
    "            expanded in the feed-forward network. The feed-forward network expands the \n",
    "            dimensionality of the input from `embed_size` to `forward_expansion * embed_size`\n",
    "            and then reduces it back to `embed_size`.\n",
    "\n",
    "    Attributes:\n",
    "        self_attention (SelfAttention): The self-attention layer used in the transformer block.\n",
    "        norm1 (nn.LayerNorm): The first layer normalization used to stabilize the outputs of \n",
    "            the self-attention layer.\n",
    "        norm2 (nn.LayerNorm): The second layer normalization used to stabilize the outputs \n",
    "            of the feed-forward network.\n",
    "        feed_forward (nn.Sequential): The feed-forward network used to transform the output \n",
    "            of the self-attention layer.\n",
    "        dropout (nn.Dropout): The dropout layer used to prevent overfitting.\n",
    "\n",
    "    The Transformer Block consists of a self-attention layer followed by normalization, \n",
    "    a feed-forward network followed by normalization, and a dropout layer. \n",
    "    \"\"\"    \n",
    "    def __init__(self, embed_size, num_heads, dropout_rate, forward_expansion, device=None):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        if device is None:\n",
    "            self.device = get_device()\n",
    "        else:\n",
    "            self.device = device\n",
    "        # Transformer block consists of a self-attention layer and a feed-forward neural network\n",
    "        self.self_attention = MultiHeadSelfAttentionBlock(embed_size, num_heads,device=device)\n",
    "        \n",
    "        # Normalization layers are used to stabilize the outputs of self-attention and feed-forward network\n",
    "        self.norm1 = nn.LayerNorm(embed_size)\n",
    "        self.norm2 = nn.LayerNorm(embed_size)\n",
    "\n",
    "        # Feed-forward neural network - it's used to transform the output of the self-attention layer\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(embed_size, forward_expansion * embed_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(forward_expansion * embed_size, embed_size),\n",
    "        )\n",
    "\n",
    "        # Dropout is used to reduce overfitting\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, values, keys, queries, mask):\n",
    "        \"\"\"\n",
    "        Forward pass of the Transformer Block.\n",
    "\n",
    "        Args:\n",
    "            values (torch.Tensor): The values used by the self-attention layer. \n",
    "                They have shape (N, value_len, embed_size) where N is the batch size, \n",
    "                value_len is the length of the value sequence, and embed_size is the size of the embeddings.\n",
    "            keys (torch.Tensor): The keys used by the self-attention layer. \n",
    "                They have shape (N, key_len, embed_size) where N is the batch size, \n",
    "                key_len is the length of the key sequence, and embed_size is the size of the embeddings.\n",
    "            queries (torch.Tensor): The queries used by the self-attention layer. \n",
    "                They have shape (N, query_len, embed_size) where N is the batch size, \n",
    "                query_len is the length of the query sequence, and embed_size is the size of the embeddings.\n",
    "            mask (torch.Tensor): The mask to be applied on the attention outputs to prevent the model \n",
    "                from attending to certain positions. It has shape (N, 1, 1, src_len), \n",
    "                where N is the batch size and src_len is the source sequence length.\n",
    "\n",
    "        Returns:\n",
    "            out (torch.Tensor): The output tensor from the transformer block, it has shape \n",
    "                (N, query_len, embed_size), where N is the batch size, query_len is the length \n",
    "                of the query sequence, and embed_size is the size of the embeddings.\n",
    "\n",
    "        The forward method first applies the self-attention mechanism on the input tensor using the provided\n",
    "        keys, queries, and values. The output from the self-attention layer is then passed through a\n",
    "        normalization layer and a dropout layer. The output from these layers is then passed through the\n",
    "        feed-forward network. The output from the feed-forward network is also passed through a \n",
    "        normalization layer and a dropout layer. The final output is then returned.\n",
    "        \"\"\"\n",
    "        values = values.to(self.device)\n",
    "        keys = keys.to(self.device)\n",
    "        queries = queries.to(self.device)\n",
    "        mask = mask.to(self.device) if mask is not None else None\n",
    "        \n",
    "        # Self-attention layer takes in values, keys and queries and returns an output tensor\n",
    "        attention_output = self.self_attention(values, keys, queries, mask)\n",
    "\n",
    "        # Add residual connection (skip connection), normalize and apply dropout\n",
    "        # The normalization is applied on the sum of the original input `queries` and the output of the self-attention layer\n",
    "        x = self.dropout(self.norm1(attention_output + queries))\n",
    "\n",
    "        # Pass the output from the attention layer through the feed-forward network\n",
    "        ff_output = self.feed_forward(x)\n",
    "        \n",
    "        # Add another residual connection, normalize and apply dropout\n",
    "        # The normalization is applied on the sum of the previous output `x` and the output of the feed-forward network\n",
    "        out = self.dropout(self.norm2(ff_output + x))\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".....\n",
      "----------------------------------------------------------------------\n",
      "Ran 5 tests in 0.149s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class TestTransformerBlock(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.embed_size = 512\n",
    "        self.num_heads = 8\n",
    "        self.dropout_rate = 0.1\n",
    "        self.forward_expansion = 4\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            self.embed_size, self.num_heads, self.dropout_rate, self.forward_expansion\n",
    "        )\n",
    "        self.transformer_block.to(get_device())\n",
    "\n",
    "    def test_forward_pass(self):\n",
    "        N = 64  # batch size\n",
    "        seq_len = 10  # sequence length\n",
    "\n",
    "        values = Variable(torch.rand(N, seq_len, self.embed_size))\n",
    "        keys = Variable(torch.rand(N, seq_len, self.embed_size))\n",
    "        queries = Variable(torch.rand(N, seq_len, self.embed_size))\n",
    "        mask = Variable(torch.ones(N, 1, 1, seq_len))\n",
    "\n",
    "        out = self.transformer_block(values, keys, queries, mask)\n",
    "        \n",
    "        # Assert that the output has the correct type and shape\n",
    "        self.assertIsInstance(out, torch.Tensor)\n",
    "        self.assertTupleEqual(out.shape, (N, seq_len, self.embed_size))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder\n",
    "\n",
    "The forward pass in the Encoder is as follows:\n",
    "\n",
    "1. **Tokens Sequence**: The process begins with a sequence of tokens. The shape of this sequence is typically (N, SequenceLength), where N is the batch size and SequenceLength is the length of the sequence for each batch.\n",
    "\n",
    "1. **Word Embeddings Mapping**: The tokens are then passed through an embedding layer that maps each token to a high-dimensional vector. This results in a tensor with shape (N, SequenceLength, E), where E is the dimensionality of the embeddings.\n",
    "\n",
    "1. **Positional Encoding**: Given the lack of inherent sequential information in a Transformer's architecture, a positional encoding step is applied. This adds information about the position of each token in the sequence to the embedding. The output of this step still maintains the shape (N, SequenceLength, E).\n",
    "\n",
    "1. **Dropout**: To prevent overfitting and improve generalization, a dropout layer is applied. Dropout randomly sets a fraction of input units to 0 at each update during training. The shape remains (N, SequenceLength, E) after this layer as well.\n",
    "\n",
    "1. **Transformer Block**: The processed embeddings are then passed through a Transformer block. The block consists of a self-attention mechanism and a feed-forward neural network, both followed by normalization and dropout. This process could be repeated for a specific number of layers (as Transformers usually consist of multiple such blocks stacked upon each other).\n",
    "\n",
    "1. **Output**: The final output from the Transformer block still maintains the shape (N, SequenceLength, E). This can be used as the input to the next Transformer block in the sequence or to a final linear layer to obtain prediction scores for tasks like sequence classification or translation.\n",
    "\n",
    "\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "X[Tokens Sequence] --shape=N,SeqLength--> W[Word Embeddings Mapping]\n",
    "W --shape=N,SeqLength,E--> PE[Positional Encoding]\n",
    "PE --shape=N,SeqLength,E--> DO[Dropout]\n",
    "DO --shape=N,SeqLength,E--> T[Transformer Block]\n",
    "T --Number of Layers Times--> T\n",
    "T --shape=N,SeqLength,E--> O[Output]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The Encoder class for a Transformer model.\n",
    "    \n",
    "    Args:\n",
    "        src_vocab_size (int): The size of the source vocabulary.\n",
    "        embed_size (int): The dimensionality of the input embeddings.\n",
    "        num_layers (int): The number of layers in the transformer.\n",
    "        num_heads (int): The number of attention heads in the transformer block.\n",
    "        device (torch.device): The device to run the model on (CPU or GPU).\n",
    "        forward_expansion (int): The expansion factor for the feed forward network in transformer block.\n",
    "        dropout (float): The dropout rate used in the dropout layers to prevent overfitting.\n",
    "        max_length (int): The maximum sequence length the model can handle.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        num_heads,\n",
    "        forward_expansion,\n",
    "        dropout_rate,\n",
    "        max_length,\n",
    "        device=None,\n",
    "    ):\n",
    "\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        if device is None:\n",
    "            self.device = get_device()\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        # Embeddings for the input words and positional embeddings\n",
    "        self.word_embedding = nn.Embedding(src_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        # Transformer blocks for the encoder layers\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                TransformerBlock(\n",
    "                    embed_size,\n",
    "                    num_heads,\n",
    "                    dropout_rate=dropout_rate,\n",
    "                    forward_expansion=forward_expansion,\n",
    "                    device=self.device,\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        Forward method for the Encoder class.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, seq_length).\n",
    "            mask (torch.Tensor): The mask to be applied on the attention outputs to prevent the model \n",
    "                from attending to certain positions.\n",
    "        \n",
    "        Returns:\n",
    "            out (torch.Tensor): The output tensor from the encoder.\n",
    "        \"\"\"\n",
    "\n",
    "        # Obtain the batch size and sequence length\n",
    "        N, seq_length = x.shape\n",
    "        x = x.to(self.device)\n",
    "        mask = mask.to(self.device)\n",
    "\n",
    "        # Create positional indices\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "\n",
    "        # Combine word embeddings and positional embeddings\n",
    "        embeddings = self.word_embedding(x) + self.position_embedding(positions)\n",
    "\n",
    "        # Apply dropout to the combined embeddings\n",
    "        out = self.dropout(embeddings)\n",
    "\n",
    "        # Pass the output through the layers\n",
    "        for layer in self.layers:\n",
    "            out = layer(out, out, out, mask)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "......\n",
      "----------------------------------------------------------------------\n",
      "Ran 6 tests in 0.255s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class TestEncoder(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.src_vocab_size = 1000\n",
    "        self.embed_size = 512\n",
    "        self.num_layers = 6\n",
    "        self.num_heads = 8\n",
    "        self.forward_expansion = 4\n",
    "        self.dropout_rate = 0.1\n",
    "        self.max_length = 5000\n",
    "\n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size=self.src_vocab_size,\n",
    "            embed_size=self.embed_size,\n",
    "            num_layers=self.num_layers,\n",
    "            num_heads=self.num_heads,\n",
    "            forward_expansion=self.forward_expansion,\n",
    "            dropout_rate=self.dropout_rate,\n",
    "            max_length=self.max_length\n",
    "        )\n",
    "\n",
    "        self.encoder.to(get_device())\n",
    "\n",
    "    def test_forward(self):\n",
    "        batch_size = 32\n",
    "        seq_length = 10\n",
    "        src_vocab = torch.randint(0, self.src_vocab_size, (batch_size, seq_length)).to(get_device())\n",
    "        src_mask = torch.zeros((batch_size, 1, 1, seq_length)).to(get_device())\n",
    "        \n",
    "        out = self.encoder(src_vocab, src_mask)\n",
    "\n",
    "        self.assertTupleEqual(out.shape, (batch_size, seq_length, self.embed_size))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder Block\n",
    "\n",
    "The decoder block uses the same components as the encoder block, such as Multi-head attention, Layer Normalization, and Feed-Forward Neural Network. However, it also has a few additional components, such as the encoder-decoder attention block and the target mask. The forward pass in the Decoder block is as follows:\n",
    "\n",
    "\n",
    "1. **Target Mask & X embeddings**: The decoder block starts by receiving the target mask and the embeddings of the target sequence. The target mask is used to prevent the model from peaking at future positions in the sequence, enforcing the auto-regressive property. The shape of the embeddings (X) is (N, S, E), where N is the batch size, S is the sequence length, and E is the embedding size.\n",
    "\n",
    "1. **MultiHeadSelfAttentionBlock** (MHA): The embeddings and the target mask are passed to a MultiHeadSelfAttentionBlock. This module applies self-attention to the embeddings, taking into account the target mask to avoid peaking at future positions.\n",
    "\n",
    "1. **LayerNorm + Dropout (LN1**): The output of the self-attention block (which still has shape (N, S, E)) is then normalized and subjected to dropout to prevent overfitting. The input embeddings X are also added back to the output of the self-attention block, in a \"residual connection\" that helps to mitigate the vanishing gradients problem in deep networks.\n",
    "\n",
    "1. **Transformer Block (TR)**: The output from the previous layer serves as the queries to the Transformer Block. It also receives the keys (K) and values (V), which typically are the outputs of the encoder. Additionally, the source mask is applied, which prevents the decoder from attending to specific positions in the source sequence (for example, padding positions).\n",
    "\n",
    "1. **Output (O)**: Finally, the output from the Transformer Block is forwarded. This is typically passed through a linear layer followed by a softmax to produce probability scores for each token in the target vocabulary. The shape of this output is still (N, S, E), maintaining the batch size, sequence length, and embedding size dimensions.\n",
    "\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "T[Target Mask] --> MHA[MultiHeadSelfAttentionBlock]\n",
    "X[X embeddings] --shape=N,S,E--> MHA\n",
    "\n",
    "MHA --shape=N,S,E--> LN1[LayerNorm + Dropout]\n",
    "X --shape=N,S,E--> LN1\n",
    "LN1 --queries=N,Sq,E--> TR[Transformer Block]\n",
    "V[Values] --shape=N,Sv,E--> TR\n",
    "K[Keys] --shape=N,Sk,E--> TR\n",
    "S[Source Mask] --> TR\n",
    "TR --shape=N,S,E--> O[Output]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    The DecoderBlock class that forms a part of the Decoder in a Transformer model.\n",
    "\n",
    "    Args:\n",
    "        embed_size (int): The dimensionality of the input embeddings.\n",
    "        num_heads (int): The number of attention heads for the self-attention mechanism.\n",
    "        forward_expansion (int): The factor by which the dimensionality of the input is expanded in the feed-forward network.\n",
    "        dropout_rate (float): The dropout rate used in the dropout layers to prevent overfitting.\n",
    "        device (torch.device): The device to run the model on (CPU or GPU).\n",
    "        \n",
    "    Attributes:\n",
    "        norm (nn.LayerNorm): Layer normalization.\n",
    "        attention (SelfAttention): The self-attention mechanism.\n",
    "        transformer_block (TransformerBlock): A transformer block.\n",
    "        dropout (nn.Dropout): Dropout layer for regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_size, num_heads, forward_expansion, dropout_rate, device=None):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        if device is None:\n",
    "            self.device = get_device()\n",
    "        else:\n",
    "            self.device = device        \n",
    "        self.norm = nn.LayerNorm(embed_size)\n",
    "        self.self_attention = MultiHeadSelfAttentionBlock(embed_size, num_heads)\n",
    "        self.transformer_block = TransformerBlock(\n",
    "            embed_size, num_heads, dropout_rate, forward_expansion\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, value, key, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        Forward method for the DecoderBlock class.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor.\n",
    "            value (torch.Tensor): The values to be used in the self-attention mechanism.\n",
    "            key (torch.Tensor): The keys to be used in the self-attention mechanism.\n",
    "            src_mask (torch.Tensor): The source mask to prevent attention to certain positions.\n",
    "            trg_mask (torch.Tensor): The target mask to prevent attention to certain positions.\n",
    "        \n",
    "        Returns:\n",
    "            out (torch.Tensor): The output tensor from the transformer block.\n",
    "        \"\"\"\n",
    "\n",
    "        x = x.to(self.device)\n",
    "        value = value.to(self.device)\n",
    "        key = key.to(self.device)\n",
    "        src_mask = src_mask.to(self.device)\n",
    "        trg_mask = trg_mask.to(self.device)\n",
    "        \n",
    "        # Compute self attention\n",
    "        attention = self.self_attention(x, x, x, trg_mask)\n",
    "        \n",
    "        # Apply normalization and dropout to the sum of the input and the attention output\n",
    "        query = self.dropout(self.norm(attention + x))\n",
    "        \n",
    "        # Compute the output of the transformer block\n",
    "        out = self.transformer_block(value, key, query, src_mask)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".......\n",
      "----------------------------------------------------------------------\n",
      "Ran 7 tests in 0.318s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class TestDecoderBlock(unittest.TestCase):\n",
    "\n",
    "    def setUp(self):\n",
    "        self.batch_size = 64\n",
    "        self.seq_length = 50\n",
    "        self.embed_size = 512\n",
    "        self.num_heads = 8\n",
    "        self.forward_expansion = 4\n",
    "        self.dropout_rate = 0.1\n",
    "\n",
    "        # Initialize an instance of the DecoderBlock\n",
    "        self.decoder_block = DecoderBlock(self.embed_size, self.num_heads, self.forward_expansion, self.dropout_rate)\n",
    "        self.decoder_block.to(get_device())\n",
    "\n",
    "        # Initialize some random test data\n",
    "        self.x = torch.randn(self.batch_size, self.seq_length, self.embed_size)\n",
    "        self.value = torch.randn(self.batch_size, self.seq_length, self.embed_size)\n",
    "        self.key = torch.randn(self.batch_size, self.seq_length, self.embed_size)\n",
    "        self.src_mask = torch.randn(self.batch_size, 1, 1, self.seq_length)\n",
    "        self.trg_mask = torch.randn(self.batch_size, 1, 1, self.seq_length)\n",
    "\n",
    "    def test_decoder_block(self):\n",
    "        # Forward pass through the DecoderBlock\n",
    "        output = self.decoder_block(self.x, self.value, self.key, self.src_mask, self.trg_mask)\n",
    "\n",
    "        # Assert the output shape is as expected\n",
    "        self.assertEqual(output.shape, (self.batch_size, self.seq_length, self.embed_size))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder\n",
    "\n",
    "In decoder the forward pass is as follows:\n",
    "\n",
    "1. **Tokens Sequence**: The target sequence, denoted as X, is the input to the decoder. Its shape is (N, SeqLength), where N is the batch size and SeqLength is the sequence length.\n",
    "\n",
    "1. **Word Embeddings Mapping**: The input sequence X is passed through an embedding layer that transforms each token into a dense vector of size E. The resulting shape is (N, SeqLength, E), where E is the embedding size.\n",
    "\n",
    "1. **Positional Encoding**: Positional encoding is added to these embeddings to encode the order of the tokens in the sequence, since this information is not inherently captured in the self-attention mechanism.\n",
    "\n",
    "1. **Dropout**: A dropout layer is applied to prevent overfitting and to add some noise to the data.\n",
    "\n",
    "1. **As X embeddings sequence** (identity): The output from the dropout layer serves as the initial sequence of embeddings X for the decoder block.\n",
    "\n",
    "1. **Target Encoded Sequence**: The encoded sequence (Y), typically the output of the encoder, is processed to be used as the keys and values for the attention mechanism in the decoder. They are of shape (N, SeqLength, E).\n",
    "\n",
    "1. **As Key/Value embeddings sequence** (identity): The encoded sequence is used directly as keys (ASK) and values (ASV), with the same shape of (N, SeqLength, E).\n",
    "\n",
    "1. **Decoder Block**: The decoder block receives the input embeddings sequence X, the key embeddings ASK, and the value embeddings ASV. The decoder block applies masked self-attention on X and then applies another attention mechanism using X as queries, ASK as keys and ASV as values.The Decoder Block's output is then passed through the same series of layers multiple times (the exact number is a parameter of the model), with each pass allowing the decoder to refine its understanding of the input sequence in the context of the output it is generating.\n",
    "\n",
    "1. **Output**: The final output of the Decoder Block, which retains the shape of (N, SeqLength, E), can be used to predict the next token in the target sequence by passing it through a linear layer and then a softmax to generate probability scores for each token in the target vocabulary.\n",
    "\n",
    "```mermaid\n",
    "graph TB\n",
    "X[Tokens Sequence: X] --shape=N,SeqLength-->  WE[Word Embeddings Mapping]\n",
    "WE --shape=N,SeqLength,E--> PE[Positional Encoding]\n",
    "PE --shape=N,SeqLength,E--> DO[Dropout]\n",
    "DO --shape=N,SeqLength,E--> ASX[As X embeddings sequence]\n",
    "ASX --shape=N,SeqLength,E-->T[Decoder Block]\n",
    "T --Number of Layers Times--> T\n",
    "Y[Target Encoded Sequence] --shape=N,SeqLength,E --> ASV[As Value embeddings sequence]\n",
    "Y --shape=N,SeqLength,E--> ASK[As Key embeddings sequence]\n",
    "ASK --shape=N,SeqLength,E-->T[Decoder Block]\n",
    "ASV --shape=N,SeqLength,E-->T[Decoder Block]\n",
    "T --> O[Output]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    The Decoder class for a Transformer model.\n",
    "    \n",
    "    Args:\n",
    "        trg_vocab_size (int): The size of the target vocabulary.\n",
    "        embed_size (int): The dimensionality of the input embeddings.\n",
    "        num_layers (int): The number of layers in the transformer.\n",
    "        num_heads (int): The number of attention heads in the transformer block.\n",
    "        forward_expansion (int): The expansion factor for the feed forward network in transformer block.\n",
    "        dropout_rate (float): The dropout rate used in the dropout layers to prevent overfitting.\n",
    "        device (torch.device): The device to run the model on (CPU or GPU).\n",
    "        max_length (int): The maximum sequence length the model can handle.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        trg_vocab_size,\n",
    "        embed_size,\n",
    "        num_layers,\n",
    "        num_heads,\n",
    "        forward_expansion,\n",
    "        dropout_rate,\n",
    "        max_length,\n",
    "        device = None\n",
    "    ):\n",
    "\n",
    "        super(Decoder, self).__init__()\n",
    "        if device is None:\n",
    "            self.device = get_device()\n",
    "        else:\n",
    "            self.device = device\n",
    "\n",
    "        # Embeddings for the input words and positional embeddings\n",
    "        self.word_embedding = nn.Embedding(trg_vocab_size, embed_size)\n",
    "        self.position_embedding = nn.Embedding(max_length, embed_size)\n",
    "\n",
    "        # Decoder blocks for the decoder layers\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                DecoderBlock(\n",
    "                    embed_size,\n",
    "                    num_heads,\n",
    "                    forward_expansion,\n",
    "                    dropout_rate,\n",
    "                    device=self.device\n",
    "                )\n",
    "                for _ in range(num_layers)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Fully connected layer to map the decoder's output to the target vocabulary size\n",
    "        self.fc_out = nn.Linear(embed_size, trg_vocab_size)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x, enc_out, src_mask, trg_mask):\n",
    "        \"\"\"\n",
    "        Forward method for the Decoder class.\n",
    "        \n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor of shape (batch_size, seq_length).\n",
    "            enc_out (torch.Tensor): The output from the encoder.\n",
    "            src_mask (torch.Tensor): The source mask to prevent the model from attending to certain positions.\n",
    "            trg_mask (torch.Tensor): The target mask to prevent the model from attending to certain positions.\n",
    "        \n",
    "        Returns:\n",
    "            out (torch.Tensor): The output tensor from the decoder.\n",
    "        \"\"\"\n",
    "\n",
    "        # Obtain the batch size and sequence length\n",
    "        N, seq_length = x.shape\n",
    "        x = x.to(self.device)\n",
    "        enc_out = enc_out.to(self.device)\n",
    "        src_mask = src_mask.to(self.device)\n",
    "        trg_mask = trg_mask.to(self.device)\n",
    "\n",
    "        # Create positional indices\n",
    "        positions = torch.arange(0, seq_length).expand(N, seq_length).to(self.device)\n",
    "\n",
    "        # Combine word embeddings and positional embeddings\n",
    "        embeddings = self.word_embedding(x) + self.position_embedding(positions)\n",
    "\n",
    "        # Apply dropout to the combined embeddings\n",
    "        x = self.dropout(embeddings)\n",
    "\n",
    "        # Pass the output through the layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_out, enc_out, src_mask, trg_mask)\n",
    "\n",
    "        # Apply the fully connected layer to the outputs of the layers\n",
    "        out = self.fc_out(x)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E.......\n",
      "======================================================================\n",
      "ERROR: test_forward (__main__.TestDecoder)\n",
      "----------------------------------------------------------------------\n",
      "Traceback (most recent call last):\n",
      "  File \"/var/folders/kh/ts9l8kk51cg14948m227s7f80000gp/T/ipykernel_27732/1802322832.py\", line 26, in setUp\n",
      "    self.src_mask = Transformer.make_src_mask(self.x, 0)\n",
      "NameError: name 'Transformer' is not defined\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "Ran 8 tests in 0.538s\n",
      "\n",
      "FAILED (errors=1)\n"
     ]
    }
   ],
   "source": [
    "class TestDecoder(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.batch_size = 64\n",
    "        self.seq_length = 50\n",
    "        self.embed_size = 512\n",
    "        self.num_heads = 8\n",
    "        self.num_layers = 8\n",
    "        self.forward_expansion = 4\n",
    "        self.dropout_rate = 0.1\n",
    "        self.trg_vocab_size = 10000\n",
    "        self.max_length = 1000\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            self.trg_vocab_size,\n",
    "            self.embed_size,\n",
    "            self.num_layers,\n",
    "            self.num_heads,\n",
    "            self.forward_expansion,\n",
    "            self.dropout_rate,\n",
    "            self.max_length\n",
    "        )\n",
    "        self.decoder.to(get_device())\n",
    "\n",
    "        self.x = torch.randint(0, self.trg_vocab_size, (self.batch_size, self.seq_length))\n",
    "        self.enc_out = torch.randn(self.batch_size, self.seq_length, self.embed_size)\n",
    "        self.src_mask = Transformer.make_src_mask(self.x, 0)\n",
    "        torch.ones((self.batch_size, 1, 1, self.seq_length))\n",
    "        self.trg_mask = Transformer.make_trg_mask(self.x)\n",
    "\n",
    "    def test_forward(self):\n",
    "        # Perform a forward pass through the decoder\n",
    "        output = self.decoder(self.x, self.enc_out, self.src_mask, self.trg_mask)\n",
    "\n",
    "        # Check the output size\n",
    "        self.assertTupleEqual(output.size(), (self.batch_size, self.seq_length, self.trg_vocab_size))\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    The Transformer model class that combines an Encoder and a Decoder.\n",
    "\n",
    "    Args:\n",
    "        src_vocab_size (int): The size of the source vocabulary.\n",
    "        trg_vocab_size (int): The size of the target vocabulary.\n",
    "        src_pad_idx (int): The index of the source padding token in the source vocabulary.\n",
    "        trg_pad_idx (int): The index of the target padding token in the target vocabulary.\n",
    "        embed_size (int): The dimensionality of the input embeddings.\n",
    "        num_layers (int): The number of layers in the transformer.\n",
    "        forward_expansion (int): The expansion factor for the feed forward network in transformer block.\n",
    "        heads (int): The number of attention heads in the transformer block.\n",
    "        dropout (float): The dropout rate used in the dropout layers to prevent overfitting.\n",
    "        device (torch.device): The device to run the model on (CPU or GPU).\n",
    "        max_length (int): The maximum sequence length the model can handle.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        src_vocab_size,\n",
    "        trg_vocab_size,\n",
    "        src_pad_idx,\n",
    "        trg_pad_idx,\n",
    "        embed_size=512,\n",
    "        num_layers=6,\n",
    "        forward_expansion=4,\n",
    "        heads=8,\n",
    "        dropout=0,\n",
    "        max_length=100,\n",
    "        device=None\n",
    "    ):\n",
    "\n",
    "        super(Transformer, self).__init__()\n",
    "        if device is None:\n",
    "            self.device = get_device()\n",
    "        else:\n",
    "            self.device = device  \n",
    "        self.encoder = Encoder(\n",
    "            src_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.decoder = Decoder(\n",
    "            trg_vocab_size,\n",
    "            embed_size,\n",
    "            num_layers,\n",
    "            heads,\n",
    "            forward_expansion,\n",
    "            dropout,\n",
    "            max_length,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "        self.trg_pad_idx = trg_pad_idx\n",
    "        self.device = device\n",
    "\n",
    "    @staticmethod\n",
    "    def make_src_mask(src,src_pad_idx,device=None):\n",
    "        \"\"\"\n",
    "        Creates a mask for the source input sequence.\n",
    "        \n",
    "        Args:\n",
    "            src (torch.Tensor): The source input sequence.\n",
    "        \n",
    "        Returns:\n",
    "            src_mask (torch.Tensor): The mask for the source input sequence.\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            device = get_device()\n",
    "        src_mask = (src != src_pad_idx).unsqueeze(1).unsqueeze(2).to(device)\n",
    "        return src_mask\n",
    "\n",
    "    @staticmethod\n",
    "    def make_trg_mask(trg,device=None):\n",
    "        \"\"\"\n",
    "        Creates a mask for the target input sequence.\n",
    "        \n",
    "        Args:\n",
    "            trg (torch.Tensor): The target input sequence.\n",
    "        \n",
    "        Returns:\n",
    "            trg_mask (torch.Tensor): The mask for the target input sequence.\n",
    "        \"\"\"\n",
    "        if device is None:\n",
    "            device = get_device()\n",
    "        N, trg_len = trg.shape\n",
    "        trg_mask = torch.tril(torch.ones((trg_len, trg_len))).expand(\n",
    "            N, 1, trg_len, trg_len\n",
    "        ).to(device)\n",
    "        \n",
    "        return trg_mask\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        \"\"\"\n",
    "        Forward method for the Transformer class.\n",
    "        \n",
    "        Args:\n",
    "            src (torch.Tensor): The source input sequence.\n",
    "            trg (torch.Tensor): The target input sequence.\n",
    "        \n",
    "        Returns:\n",
    "            out (torch.Tensor): The output tensor from the transformer.\n",
    "        \"\"\"\n",
    "        src = src.to(self.device)\n",
    "        trg = trg.to(self.device)\n",
    "        src_mask = self.make_src_mask(src,self.src_pad_idx,self.device)\n",
    "        trg_mask = self.make_trg_mask(trg,self.device)\n",
    "        enc_src = self.encoder(src, src_mask)\n",
    "        out = self.decoder(trg, enc_src, src_mask, trg_mask)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".........\n",
      "----------------------------------------------------------------------\n",
      "Ran 9 tests in 1.048s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "class TestTransformer(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.src_pad_idx = 0\n",
    "        self.trg_pad_idx = 0\n",
    "        self.src_vocab_size = 10\n",
    "        self.trg_vocab_size = 10\n",
    "        self.model = Transformer(self.src_vocab_size, self.trg_vocab_size, self.src_pad_idx, self.trg_pad_idx)\n",
    "        self.model.to(get_device())\n",
    "\n",
    "        self.x = torch.tensor([[1, 5, 6, 4, 3, 9, 5, 2, 0], [1, 8, 7, 3, 4, 5, 6, 7, 2]]).to(get_device())\n",
    "        self.trg = torch.tensor([[1, 7, 4, 3, 5, 9, 2, 0], [1, 5, 6, 2, 4, 7, 6, 2]]).to(get_device())\n",
    "\n",
    "    def test_forward_pass(self):\n",
    "        out = self.model(self.x, self.trg[:, :-1])\n",
    "        self.assertTupleEqual(out.shape, (self.trg.shape[0], self.trg.shape[1] - 1, self.trg_vocab_size))\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=['first-arg-is-ignored'], exit=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fundaml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
